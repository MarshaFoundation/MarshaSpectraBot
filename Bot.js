const TelegramBot = require('node-telegram-bot-api');
const i18n = require('i18n');
const wtf = require('wtf_wikipedia');
const axios = require('axios');
require('dotenv').config();

const token = process.env.TELEGRAM_API_KEY;
const openaiApiKey = process.env.OPENAI_API_KEY;

// Configuraci贸n del objeto de configuraci贸n
const CONFIG = {
    locales: ['en', 'es'],
    defaultLocale: 'es',
    cacheMaxSize: 100,
    openaiApiUrl: 'https://api.openai.com/v1/chat/completions',
    gptModel: 'gpt-3.5-turbo',
    responseTemperature: 0.7,
};

i18n.configure({
    locales: CONFIG.locales,
    directory: __dirname + '/locales',
    defaultLocale: CONFIG.defaultLocale,
    queryParameter: 'lang',
    cookie: 'locale',
});

const bot = new TelegramBot(token, { polling: true });
console.log('Bot iniciado correctamente');

// Implementaci贸n de una cach茅 LRU
class LRUCache {
    constructor(maxSize) {
        this.maxSize = maxSize;
        this.cache = new Map();
    }

    get(key) {
        if (!this.cache.has(key)) {
            return null;
        }
        const value = this.cache.get(key);
        this.cache.delete(key);
        this.cache.set(key, value);
        return value;
    }

    set(key, value) {
        if (this.cache.size >= this.maxSize) {
            const keys = this.cache.keys();
            this.cache.delete(keys.next().value);
        }
        this.cache.set(key, value);
    }
}

const cache = new LRUCache(CONFIG.cacheMaxSize);

// Funci贸n para agregar retrasos
const delay = ms => new Promise(resolve => setTimeout(resolve, ms));

// Funci贸n para hacer la llamada a OpenAI con control de frecuencia
async function getChatGPTResponse(messages) {
    const messagesKey = JSON.stringify(messages);
    const cachedResponse = cache.get(messagesKey);
    if (cachedResponse) {
        return cachedResponse;
    }

    const maxRetries = 5;
    let retries = 0;

    while (retries < maxRetries) {
        try {
            const response = await axios.post(CONFIG.openaiApiUrl, {
                model: CONFIG.gptModel,
                messages: messages,
                temperature: CONFIG.responseTemperature,
            }, {
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${openaiApiKey}`
                }
            });

            const gptResponse = response.data.choices[0].message.content.trim();
            cache.set(messagesKey, gptResponse);

            return gptResponse;
        } catch (error) {
            if (error.response && error.response.status === 429) {
                console.warn('Rate limit reached. Waiting to retry...');
                retries++;
                await delay(5000); // Incrementar el tiempo de retraso a 5 segundos
            } else {
                console.error('Error al llamar a OpenAI:', error);
                return 'Lo siento, actualmente no puedo procesar tu solicitud.';
            }
        }
    }

    return 'Lo siento, actualmente no puedo procesar tu solicitud.';
}

// Nueva funci贸n de prueba de conexi贸n
async function testOpenAiConnection() {
    const testMessage = "Dime un hecho curioso.";
    const prompt = { role: 'user', content: testMessage };
    const messages = [prompt];

    try {
        const response = await getChatGPTResponse(messages);

        if (response) {
            console.log("Conexi贸n a OpenAI exitosa:", response);
        } else {
            console.error("No se recibi贸 una respuesta v谩lida de OpenAI.");
        }
    } catch (error) {
        console.error("Error en la conexi贸n a OpenAI:", error);
    }
}

// Llama a la funci贸n de prueba al inicio para verificar la conexi贸n
testOpenAiConnection();

// Funciones de utilidad
async function handleError(chatId, errorMessage, errorDetails = '') {
    console.error(errorMessage, errorDetails);
    await bot.sendMessage(chatId, i18n.__('Ha ocurrido un error. Por favor, int茅ntalo nuevamente m谩s tarde.'));
}

function sanitizeInput(input) {
    return input.replace(/[^a-zA-Z0-9谩茅铆贸煤眉帽\s.,?!]/g, '');
}

// Bot commands setup
bot.onText(/\/start/, async (msg) => {
    const chatId = msg.chat.id;
    const opts = {
        reply_markup: JSON.stringify({
            inline_keyboard: [
                [{ text: ' English', callback_data: 'en' }],
                [{ text: ' Espa帽ol', callback_data: 'es' }],
            ],
        }),
    };
    const locale = CONFIG.defaultLocale;
    i18n.setLocale(locale);
    bot.sendMessage(chatId, i18n.__('隆Hola! Por favor, elige tu idioma.'), opts);
});

bot.on('callback_query', async (callbackQuery) => {
    const chatId = callbackQuery.message.chat.id;
    const locale = callbackQuery.data;
    i18n.setLocale(locale);
    bot.sendMessage(chatId, i18n.__('Idioma cambiado a %s', i18n.getLocale()));
});

bot.on('message', async (msg) => {
    const chatId = msg.chat.id;
    const userMessage = sanitizeInput(msg.text);

    try {
        const prompt = { role: 'user', content: userMessage };
        const messages = [prompt];
        const gptResponse = await getChatGPTResponse(messages);

        if (!gptResponse) {
            const doc = await wtf.fetch(userMessage, 'es');
            const summary = doc && doc.sections(0).paragraphs(0).sentences(0).text();
            bot.sendMessage(chatId, summary || i18n.__('Lo siento, no entiendo eso. 驴Podr铆as reformularlo?'));
        } else {
            bot.sendMessage(chatId, gptResponse);
        }
    } catch (error) {
        await handleError(chatId, error.message, error);
    }
});

bot.on('polling_error', (error) => {
    console.error('Error de polling:', error);
});

process.on('uncaughtException', (err) => {
    console.error('Error no capturado:', err);
});

process.on('unhandledRejection', (reason, promise) => {
    console.error('Error no manejado:', reason, 'promise:', promise);
});

